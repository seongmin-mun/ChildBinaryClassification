{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"widgets":{"application/vnd.jupyter.widget-state+json":{"942eeb236a7a4ebcb301c9a3a9234367":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ae26d627f334759a6f5c3e86a9cadb3","IPY_MODEL_faf6c27d6c25401bac08868c55ec4c5a","IPY_MODEL_4346bea43e374676910f8a4f74628f50"],"layout":"IPY_MODEL_14afaf7a22904d07ace773d47697e5a9"}},"6ae26d627f334759a6f5c3e86a9cadb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f4c78433ce84ec0a0caf37713bed5e9","placeholder":"​","style":"IPY_MODEL_779ec858628e4527b619b093094fb31b","value":"Downloading: 100%"}},"faf6c27d6c25401bac08868c55ec4c5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86ea66c140fb45c6a1fa3a334297bd50","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7747c37c8d4469c84c0346777e5e0b2","value":1000}},"4346bea43e374676910f8a4f74628f50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3d70741657e4dd4950dd3c18fd76332","placeholder":"​","style":"IPY_MODEL_b92e2ebbec6f429baac27d6fc5277acb","value":" 0.98k/0.98k [00:00&lt;00:00, 32.7kB/s]"}},"14afaf7a22904d07ace773d47697e5a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f4c78433ce84ec0a0caf37713bed5e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"779ec858628e4527b619b093094fb31b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86ea66c140fb45c6a1fa3a334297bd50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7747c37c8d4469c84c0346777e5e0b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3d70741657e4dd4950dd3c18fd76332":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b92e2ebbec6f429baac27d6fc5277acb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75ce9ce8be98431a82f3d77d2c939e17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe59c955f6e34ceeadd8da5bb84ab50e","IPY_MODEL_5babdd170412424792ab05acaa6d7327","IPY_MODEL_96314a55e21e4ae78a8d62e41023a71f"],"layout":"IPY_MODEL_5ca07bd1567549d19a8436f098f7bebf"}},"fe59c955f6e34ceeadd8da5bb84ab50e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57bab99914c94c2cbcd35b1795b02654","placeholder":"​","style":"IPY_MODEL_a89b551e014846229a5fb8c48ec9ccbe","value":"Downloading: 100%"}},"5babdd170412424792ab05acaa6d7327":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0005dd66024543b5a3285194d8210605","max":513302779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_74c0c9bf241c47d4be70331d1190308c","value":513302779}},"96314a55e21e4ae78a8d62e41023a71f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23aa7b9234214e35972ecac402a856f8","placeholder":"​","style":"IPY_MODEL_7a438bb1a3dc4eba8fd002c02b336114","value":" 490M/490M [00:11&lt;00:00, 46.4MB/s]"}},"5ca07bd1567549d19a8436f098f7bebf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57bab99914c94c2cbcd35b1795b02654":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a89b551e014846229a5fb8c48ec9ccbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0005dd66024543b5a3285194d8210605":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74c0c9bf241c47d4be70331d1190308c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23aa7b9234214e35972ecac402a856f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a438bb1a3dc4eba8fd002c02b336114":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99436657d9364db180d944bb7b578b16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eff6ac292af54f31a3d2abe7f5ee56e8","IPY_MODEL_b61917d4c8e2436c88436291a9324c47","IPY_MODEL_8ece04697ba2492da174e654b1d1206c"],"layout":"IPY_MODEL_67e74b14fec14ee5955affa0b524a13d"}},"eff6ac292af54f31a3d2abe7f5ee56e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72b1a7f38290471ab18fbdfb94bb0d20","placeholder":"​","style":"IPY_MODEL_5fa6c6aade604f3c9ba6b2b515e287a9","value":"Downloading: 100%"}},"b61917d4c8e2436c88436291a9324c47":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e9d11be2edc4b46bb9fcf5759d7b8d2","max":2825034,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2e5eb3302b5d4e97ade17664d3d1c4b9","value":2825034}},"8ece04697ba2492da174e654b1d1206c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f108404b12e441018028d6fd508ca354","placeholder":"​","style":"IPY_MODEL_fcba172b5ca54ffa968bfd214dc0ca36","value":" 2.69M/2.69M [00:01&lt;00:00, 2.29MB/s]"}},"67e74b14fec14ee5955affa0b524a13d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72b1a7f38290471ab18fbdfb94bb0d20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fa6c6aade604f3c9ba6b2b515e287a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e9d11be2edc4b46bb9fcf5759d7b8d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e5eb3302b5d4e97ade17664d3d1c4b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f108404b12e441018028d6fd508ca354":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcba172b5ca54ffa968bfd214dc0ca36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"premium"},"cells":[{"cell_type":"markdown","metadata":{"id":"cFEpHz9q2TkF"},"source":["# ChildGPT-2_Bi\n","https://www.kaggle.com/baekseungyun/gpt-2-with-huggingface-pytorch<br>"]},{"cell_type":"code","metadata":{"id":"li9QWpubgHQ4"},"source":["##Parameter setting\n","setEpoch = 10\n","setLearningRate = 0.0001\n","setEpsilon = 1e-8\n","setBatch = 32\n","setMaxLength = 256\n","setSeed = 42\n","labelNumber = 2\n","setTry = 31"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9LAZ1CSc3Bm_","executionInfo":{"status":"ok","timestamp":1656286149935,"user_tz":-540,"elapsed":20144,"user":{"displayName":"Seongmin Mun","userId":"17295717412647902050"}},"outputId":"eba418de-cf1d-4592-f278-cd11a7445a02"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')\n","import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n","There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SEAoJWbt2TkL","executionInfo":{"status":"ok","timestamp":1656027839039,"user_tz":-540,"elapsed":11091,"user":{"displayName":"Seongmin Mun","userId":"17295717412647902050"}},"outputId":"7b49db43-0344-4953-aac6-6e796390c7ca"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 53.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 10.5 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 52.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"code","metadata":{"id":"Hv2_wH8A2TkN"},"source":["import tensorflow as tf\n","import torch\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for currentTry in range(1,setTry):\n","  \n","  # 1. Model and Tokenizer\n","  # https://github.com/SKT-AI/KoGPT2\n","  from transformers import set_seed, GPT2LMHeadModel, PreTrainedTokenizerFast, GPT2ForSequenceClassification, GPT2Config\n","\n","  set_seed(731) # My Birthday!, you should get train_loss: 0.773, train_acc: 0.567 in epoch 0.\n","\n","  model_config = GPT2Config.from_pretrained('skt/kogpt2-base-v2', num_labels=labelNumber) # Binary Classification\n","  model = GPT2ForSequenceClassification.from_pretrained('skt/kogpt2-base-v2', config=model_config)\n","\n","  tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n","    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n","    pad_token='<pad>', mask_token='<mask>') \n","  tokenizer.padding_side = \"left\" # Very Important\n","  tokenizer.pad_token = tokenizer.eos_token\n","\n","  print(\"Before patching: \",len(tokenizer.get_vocab()))\n","\n","  # CHILDES데이터 사용하여 사전학습 모델 튜닝\n","  entireCorpus = \"drive/My Drive/NLP/GH/AgentThemeFirst/Data/Binary/CHILDES/CHILDES_all_spellchecked_corrected.txt\"\n","  entireFr = open(entireCorpus, 'r')\n","  entireContents = entireFr.readlines()\n","  entireFr.close()\n","\n","  import re\n","\n","  entireSet = set()\n","  entireContentList = list()\n","  #공백을 기준으로 단어를 생성하고 추가하기\n","  for entireContent in entireContents:\n","      entireContent = re.sub('[^가-힣]', ' ', entireContent)\n","      entireContent = re.sub('[\\s]+', ' ', entireContent)\n","      entireContentList.append(entireContent.replace(\"\\n\",\"\"))\n","\n","      entireContentSplit = entireContent.split(\" \")\n","      for each in entireContentSplit:\n","        if each != \"\":\n","          entireSet.add(each)\n","\n","  # print(len(entireSet)) #37898\n","  wordDic = {}\n","  for eachWord in entireSet:\n","    wordDic[eachWord] = 0\n","\n","  for contentEach in entireContentList:\n","    contentEachSplit = contentEach.split(\" \")\n","    for each in contentEachSplit:\n","      if each != \"\":\n","        wordDic[each] = wordDic[each] + 1\n","\n","  wordDicSorted = dict(sorted(wordDic.items(), key=lambda x: x[1], reverse=True))\n","\n","  #사전에 추가하기\n","  countNum = 1\n","  for key, value in wordDicSorted.items():\n","    if value > 1:\n","      countNum = countNum + 1\n","      tokenizer.add_tokens([key])\n","\n","  # print(countNum) #3630   #10: 3630 #1: 17650 #2: 11848 #3: 9123 #4: 7435\n","\n","  print(\"After patching: \",len(tokenizer.get_vocab()))\n","\n","  model.resize_token_embeddings(len(tokenizer))\n","  model.config.pad_token_id = model.config.eos_token_id\n","\n","  #2. Build Dataset\n","  import os\n","  import pandas as pd\n","  from torch.utils.data import Dataset\n","\n","  class Dataset(Dataset):\n","      def __init__(self, train=True):\n","          super().__init__()\n","          self.train = train\n","          self.data = pd.read_csv(os.path.join(\"drive/My Drive/NLP/GH/AgentThemeFirst/Data/Binary/\", 'trainBi.csv' if train else 'testBi_GPT2_All.csv'))\n","      \n","      def __len__(self):\n","          return len(self.data)\n","      \n","      def __getitem__(self, index):\n","          record = self.data.iloc[index]\n","          text = record['Sentence']\n","          if self.train:\n","              return {'Sentence': text, 'label': record['Label']}\n","          else:\n","              return {'Sentence': text, 'label': '0'}\n","\n","  train_dataset = Dataset(train=True)\n","  test_dataset = Dataset(train=False)\n","  #3. Data Collator\n","  class Gpt2ClassificationCollator(object):\n","      def __init__(self, tokenizer, max_seq_len=None):\n","          self.tokenizer = tokenizer\n","          self.max_seq_len = max_seq_len\n","          \n","          return\n","      \n","      def __call__(self, sequences):\n","          texts = [sequence['Sentence'] for sequence in sequences]\n","          labels = [int(sequence['label']) for sequence in sequences]\n","          inputs = self.tokenizer(text=texts,\n","                                  return_tensors='pt',\n","                                  padding=True,\n","                                  truncation=True,\n","                                  max_length=self.max_seq_len)\n","          inputs.update({'labels': torch.tensor(labels)})\n","          \n","          return inputs\n","\n","  gpt2classificationcollator = Gpt2ClassificationCollator(tokenizer=tokenizer,\n","                                                          max_seq_len=setMaxLength)\n","\n","  #4. DataLoader\n","  from torch.utils.data import DataLoader, random_split\n","\n","  train_size = int(len(train_dataset) * 0.8)\n","  val_size = len(train_dataset) - train_size\n","  train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","  train_dataloader = DataLoader(dataset=train_dataset,\n","                                batch_size=setBatch,\n","                                shuffle=True,\n","                                collate_fn=gpt2classificationcollator)\n","  val_dataloader = DataLoader(dataset=val_dataset,\n","                              batch_size=setBatch,\n","                              shuffle=False,\n","                              collate_fn=gpt2classificationcollator)\n","  test_dataloader = DataLoader(dataset=test_dataset,\n","                              batch_size=setBatch,\n","                              shuffle=False,\n","                              collate_fn=gpt2classificationcollator)\n","\n","  #5. Optimizer & Lr Scheduler\n","  from transformers import AdamW, get_cosine_schedule_with_warmup\n","\n","  total_epochs = setEpoch\n","\n","  param_optimizer = list(model.named_parameters())\n","  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","  optimizer_grouped_parameters = [\n","      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","  ]\n","  optimizer = AdamW(optimizer_grouped_parameters,\n","                    lr=setLearningRate,\n","                    eps=setEpsilon)\n","\n","  num_train_steps = len(train_dataloader) * total_epochs\n","  num_warmup_steps = int(num_train_steps * 0.1) \n","\n","  lr_scheduler = get_cosine_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=num_warmup_steps,\n","                                                num_training_steps = num_train_steps)\n","\n","\n","  #6. Train & Validation\n","  import torch\n","\n","  def train(dataloader, optimizer, scheduler, device_):\n","      global model\n","      model.train()\n","      \n","      prediction_labels = []\n","      true_labels = []\n","      \n","      total_loss = []\n","      \n","      for batch in dataloader:\n","          true_labels += batch['labels'].numpy().flatten().tolist()\n","          batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n","          \n","          \n","          outputs = model(**batch)\n","          loss, logits = outputs[:2]\n","          logits = logits.detach().cpu().numpy()\n","          total_loss.append(loss.item())\n","          \n","          optimizer.zero_grad()\n","          loss.backward()\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # prevent exploding gradient\n","\n","          optimizer.step()\n","          scheduler.step()\n","          \n","          prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n","      \n","      return true_labels, prediction_labels, total_loss\n","\n","  def validation(dataloader, device_):\n","      global model\n","      model.eval()\n","      \n","      prediction_labels = []\n","      true_labels = []\n","\n","      embedding_outputs = []\n","      \n","      total_loss = []\n","\n","      outputs = []\n","      \n","      for batch in dataloader:\n","          true_labels += batch['labels'].numpy().flatten().tolist()\n","          batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n","          \n","          with torch.no_grad():\n","              outputs = model(**batch)\n","              loss, logits = outputs[:2]\n","              logits = logits.detach().cpu().numpy()\n","              total_loss.append(loss.item())\n","\n","              prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n","\n","              embedding_outputs += logits.tolist()\n","\n","              outputs = outputs\n","          \n","      return true_labels, prediction_labels, total_loss, outputs, embedding_outputs\n","\n","  ##############################결과구문으로출력###################################\n","  def outreault(guess):\n","      guess = int(guess)\n","      outConstruction = \"\"\n","      if guess == 0:\n","          outConstruction = \"agent-first\"\n","      elif guess == 1:\n","          outConstruction = \"theme-first\"\n","\n","      return outConstruction\n","\n","  #7. Run\n","\n","  outDir = \"drive/My Drive/NLP/GH/AgentThemeFirst/Data/Binary_updated/OutputGPT2/gpt2Bi_All_T\"+str(currentTry)+\".csv\"#\"drive/My Drive/NLP/GH/BUCLD/Data/Binary/Output/gpt2Bi_BUCLD_SL256_LR000001.csv\"\n","  f = open(outDir, 'w')\n","  f.write(\"epoch,sentence,originalLabel,predictedLabel,predictedConstruction,result\"+\"\\n\")\n","\n","  from sklearn.metrics import classification_report, accuracy_score\n","\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  model.to(device)\n","\n","  all_loss = {'train_loss': [], 'val_loss': []}\n","  all_acc = {'train_acc': [], 'val_acc': []}\n","  outputs = []\n","\n","  for epoch in range(total_epochs):\n","      y, y_pred, train_loss = train(train_dataloader, optimizer, lr_scheduler, device)\n","      train_acc = accuracy_score(y, y_pred)\n","      \n","      y, y_pred, val_loss, outputs, logits_labels = validation(val_dataloader, device)\n","      val_acc = accuracy_score(y, y_pred)\n","      \n","      all_loss['train_loss'] += train_loss\n","      all_loss['val_loss'] += val_loss\n","      \n","      all_acc['train_acc'].append(train_acc)\n","      all_acc['val_acc'].append(val_acc)\n","\n","      outputs = outputs\n","\n","      print(\"\")\n","      print('======== Epoch {:} / {:} ========'.format(epoch + 1, total_epochs))\n","      print('Training...')\n","      \n","      print(f'Epoch: {epoch}, train_loss: {torch.tensor(train_loss).mean():.3f}, train_acc: {train_acc:.3f}, val_loss: {torch.tensor(val_loss).mean():.3f}, val_acc: {val_acc:.3f}') \n","\n","      if (epoch+1) == 1 or (epoch+1) == 2 or (epoch+1) == 3 or (epoch+1) == 4 or (epoch+1) == 5 or (epoch+1) == 6 or (epoch+1) == 7 or (epoch+1) == 8 or (epoch+1) == 9 or (epoch+1) == 10 :#or (epoch+1) == 20 or (epoch+1) == 30:# or (epoch+1) == 40 or (epoch+1) == 50:\n","        # ###############################전체 모델 성능 평가########################################\n","        y, y_pred, val_loss, outputs, logits_labels = validation(test_dataloader, device)\n","        import pandas as pd\n","\n","        testFileDir = fileDir = \"drive/My Drive/NLP/GH/AgentThemeFirst/Data/Binary/testBi_All.csv\"\n","        testFr = open(testFileDir, 'r')\n","        testContents = testFr.readlines()\n","        testFr.close()\n","\n","        test = pd.DataFrame(columns=('Label1', 'Label2','Sentence'))\n","        i = 0\n","        for content in testContents:\n","            if i == 0:\n","                pass\n","            else:\n","                infos = content.split(\",\")\n","                label1 = int(infos[0])\n","                label2 = int(infos[1])\n","                sentence = infos[2].replace(\"\\n\", \"\")\n","                test.loc[i] = [label1, label2, sentence]\n","            i = i + 1\n","\n","        test['Sentence'] = test['Sentence'].str.replace(r'[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》\\\\n\\t]+', \" \", regex=True)\n","        test['Sentence'] = test['Sentence'].str.replace(r'\\t+', \" \", regex=True)\n","        test['Sentence'] = test['Sentence'].str.replace(r'[\\\\n]+', \" \", regex=True)\n","\n","        # 리뷰 문장 추출\n","        testSentences = test['Sentence']\n","\n","        totalNum = 0\n","        correctNum = 0\n","        for each in range(0, len(testSentences)):\n","            print(test['Label1'][each + 1], test['Label2'][each + 1], test['Sentence'][each + 1])\n","            print(\"y_pred\", len(y_pred))\n","            guess = str(y_pred[each])\n","            if guess == str(test['Label1'][each + 1]) or guess == str(test['Label2'][each + 1]):\n","                print(\"input: \", test['Sentence'][each + 1], \", predict: \", guess, \"(O)\")\n","                f.write(str(epoch+1) + \",\" + test['Sentence'][each + 1] + \",\" + str(test['Label1'][each + 1]) + \"or\" + str(test['Label2'][each + 1]) + \",\" + guess + \",\" + outreault(guess)+ \",1\" + \"\\n\")\n","                correctNum = correctNum + 1\n","            else:\n","                f.write(str(epoch+1) + \",\" + test['Sentence'][each + 1] + \",\" + str(test['Label1'][each + 1]) + \"or\" + str(test['Label2'][each + 1]) + \",\" + guess + \",\" + outreault(guess) + \",0\" + \"\\n\")       \n","                print(\"input: \", test['Sentence'][each + 1], \", predict: \", guess, \"(X)\")\n","            totalNum = totalNum + 1\n","\n","        print(\"totalNum: \", totalNum, \" correctNum: \", correctNum, \" accuracy: \", (correctNum/totalNum))\n","\n","  f.close()\n","\n","  print(\"\")\n","  print(\"Training complete!\")"],"metadata":{"id":"YnGhc1CZafnJ","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["942eeb236a7a4ebcb301c9a3a9234367","6ae26d627f334759a6f5c3e86a9cadb3","faf6c27d6c25401bac08868c55ec4c5a","4346bea43e374676910f8a4f74628f50","14afaf7a22904d07ace773d47697e5a9","0f4c78433ce84ec0a0caf37713bed5e9","779ec858628e4527b619b093094fb31b","86ea66c140fb45c6a1fa3a334297bd50","e7747c37c8d4469c84c0346777e5e0b2","e3d70741657e4dd4950dd3c18fd76332","b92e2ebbec6f429baac27d6fc5277acb","75ce9ce8be98431a82f3d77d2c939e17","fe59c955f6e34ceeadd8da5bb84ab50e","5babdd170412424792ab05acaa6d7327","96314a55e21e4ae78a8d62e41023a71f","5ca07bd1567549d19a8436f098f7bebf","57bab99914c94c2cbcd35b1795b02654","a89b551e014846229a5fb8c48ec9ccbe","0005dd66024543b5a3285194d8210605","74c0c9bf241c47d4be70331d1190308c","23aa7b9234214e35972ecac402a856f8","7a438bb1a3dc4eba8fd002c02b336114","99436657d9364db180d944bb7b578b16","eff6ac292af54f31a3d2abe7f5ee56e8","b61917d4c8e2436c88436291a9324c47","8ece04697ba2492da174e654b1d1206c","67e74b14fec14ee5955affa0b524a13d","72b1a7f38290471ab18fbdfb94bb0d20","5fa6c6aade604f3c9ba6b2b515e287a9","7e9d11be2edc4b46bb9fcf5759d7b8d2","2e5eb3302b5d4e97ade17664d3d1c4b9","f108404b12e441018028d6fd508ca354","fcba172b5ca54ffa968bfd214dc0ca36"]},"executionInfo":{"status":"ok","timestamp":1656042238575,"user_tz":-540,"elapsed":14344066,"user":{"displayName":"Seongmin Mun","userId":"17295717412647902050"}},"outputId":"8b78f8c6-c4bb-43b3-c22c-64d74a836e92"},"execution_count":null,"outputs":[]}]}